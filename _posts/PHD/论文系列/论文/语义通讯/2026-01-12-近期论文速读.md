---
layout: post
title: 近期论文速读
date: 2026-01-12 19:52 +0800
description: 近期进展
pin: true
math: true
mermaid: true
category:
- 语义通信
tags:
- 语义通信
published: true
sitemap: false
author: tangyu
---



## 总结下近期的一些进展：

- 学习完 Hello-Agent 的大部分核心内容，后续可能通过两个项目来实践。
- 调整思路，不再做纯文本的 Agent 语义通信。理由是：自然语言本身已是高度精炼的语义载体，进一步压缩单条消息的语义信息，其信息密度可能仍不如图像丰富，难以体现出明显优势或对比效果。因此，研究方向转向图像语义通信。
- 图像、多模态语义通信可能更有落地和发表文章的机会，Agent2Agent那种文字的的高频低容量实在不好处理。

## 近期观察的文章：

### **D$^2$-JSCC: Digital Deep Joint Source-channel Coding for Semantic Communications**

本文提出了一种面向语义通信中图像传输的数字深度联合源-信道编码框架（D^2-JSCC），旨在通过深度学习算法传输数据的语义特征，实现高效的通信。该框架包括数字源编码和信道编码的联合优化，以减少端到端的失真。源编码部分采用自适应密度模型的深度编码，根据特征分布进行编码；信道编码部分用于保护编码特征，防止信道失真。通过贝叶斯模型分析和深度神经网络的Lipschitz假设，将端到端失真表示为源和信道速率的函数。为最小化失真，提出了一种两步骤算法来控制源-信道速率。仿真结果显示，该框架相较于传统的深度联合源-信道编码表现更优，并能缓解分离式方法中常见的悬崖效应和平坦效应

>简单来说就是先信源编码，然后数字编码（**实际际信道编码（如极化码**），然后再联合信道编码起来优化。效果比jscc的好。





### **Semantic Communication-Aware Federated Fine-Tuning of Large Language Models**

该论文探讨了在语义通信视角下对大型语言模型进行联邦微调的方法。研究聚焦于结合语义通信技术与联邦学习框架，旨在优化大语言模型的分布式微调过程。通过语义通信的机制，可能关注在保护数据隐私的前提下，提升模型微调的效率或语义层面的协同性能，以解决传统联邦学习中可能存在的通信开销或语义一致性问题。



> 简单来说，就是考虑到模型太大了，需要拆分模型，然后传统的方法传输梯度等信息效率太慢，用语义通信的方式，将模型传输的梯度等信息压缩后再解压，据文章所说达到了减少了80-90%的传输量和1%的性能下降。
>
>联合训练的损失函数如下，${ h } _ { k }$和$\hat { h } _ { k }$分别是每个块的语义通信模型输入和输出，意思就是语义误差尽可能的小。
>
>总的来说这篇文章比较有意思，能够考虑大模型的传输效率。


$$
\mathcal { L } _ { \mathrm { t o t a l } } = \mathcal { L } _ { \mathrm { t a s k } } + \lambda \cdot | | h _ { k } - \hat { h } _ { k } | | ^ { 2 }
$$

### **GenSC: Generative Semantic Communication Systems Using BART-Like Mode**

当前语义交际的思维方式侧重于如何准确地接收句子。然而，只要接收到的句子和原始句子被感知为相同或相似，就可以将其视为成功的语义层面传输。因此，我们为基于类似BART模型的语义通信系统设计了一种新的架构（Lewis等人，2019），称为GenSC。所提出的GenSC在语义编码过程中进一步考虑了连续令牌之间的令牌级相关性，当令牌在传输过程中丢失或损坏时，这种双向相关性有助于在语义解码器处纠正或填充“语义相似的令牌”。仿真表明，与Xie等人（2021）和Liu等人（2022）等传统方法相比，GenSC可以大大提高低信噪比区域的双语评估替补（BLEU）和语义相似性（SS）得分，并在高信噪比地区获得更高的BLEU和SS得分。当信噪比为0dB时，GenSC在BLEU（分别为SS）方面分别比（Xie等人，2021）和（Liu等人，2022）高出约30%（分别为84%）和18%（分别为55%）。

> 简单来说，这篇文章就是用transformer对文本进行抗干扰训练，没什么特别大的创新。**这个场景主要是解决传输过程中恢复漏了的文本。**

